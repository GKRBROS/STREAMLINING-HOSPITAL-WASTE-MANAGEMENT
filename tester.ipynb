{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s) with mixed precision\n",
      "Found 27123 images belonging to 3 classes.\n",
      "Found 6779 images belonging to 3 classes.\n",
      "Stage 1: Feature extraction\n",
      "\n",
      "Epoch 1/30\n",
      "Step 10/6780, Loss: 1.7359, Accuracy: 0.2375\n",
      "Step 20/6780, Loss: 1.3998, Accuracy: 0.3063\n",
      "Step 30/6780, Loss: 0.9298, Accuracy: 0.3417\n",
      "Step 40/6780, Loss: 1.1533, Accuracy: 0.3938\n",
      "Step 50/6780, Loss: 0.8338, Accuracy: 0.4300\n",
      "Step 60/6780, Loss: 1.4761, Accuracy: 0.4510\n",
      "Step 70/6780, Loss: 0.9967, Accuracy: 0.4670\n",
      "Step 80/6780, Loss: 0.9745, Accuracy: 0.4820\n",
      "Step 90/6780, Loss: 0.9505, Accuracy: 0.4903\n",
      "Step 100/6780, Loss: 0.5936, Accuracy: 0.5025\n",
      "Step 110/6780, Loss: 0.8306, Accuracy: 0.5136\n",
      "Step 120/6780, Loss: 0.5965, Accuracy: 0.5271\n",
      "Step 130/6780, Loss: 1.2447, Accuracy: 0.5361\n",
      "Step 140/6780, Loss: 0.6057, Accuracy: 0.5437\n",
      "Step 150/6780, Loss: 0.8607, Accuracy: 0.5554\n",
      "Step 160/6780, Loss: 0.8380, Accuracy: 0.5652\n",
      "Step 170/6780, Loss: 0.7780, Accuracy: 0.5684\n",
      "Step 180/6780, Loss: 0.6232, Accuracy: 0.5708\n",
      "Step 190/6780, Loss: 0.5793, Accuracy: 0.5750\n",
      "Step 200/6780, Loss: 0.6176, Accuracy: 0.5775\n",
      "Step 210/6780, Loss: 0.8423, Accuracy: 0.5783\n",
      "Step 220/6780, Loss: 0.6001, Accuracy: 0.5844\n",
      "Step 230/6780, Loss: 0.8113, Accuracy: 0.5902\n",
      "Step 240/6780, Loss: 0.8160, Accuracy: 0.5940\n",
      "Step 250/6780, Loss: 0.7240, Accuracy: 0.5965\n",
      "Step 260/6780, Loss: 0.3961, Accuracy: 0.5993\n",
      "Step 270/6780, Loss: 0.7077, Accuracy: 0.6023\n",
      "Step 280/6780, Loss: 0.8559, Accuracy: 0.6040\n",
      "Step 290/6780, Loss: 1.0224, Accuracy: 0.6045\n",
      "Step 300/6780, Loss: 0.5804, Accuracy: 0.6081\n",
      "Step 310/6780, Loss: 0.3806, Accuracy: 0.6103\n",
      "Step 320/6780, Loss: 0.6080, Accuracy: 0.6113\n",
      "Step 330/6780, Loss: 1.0455, Accuracy: 0.6125\n",
      "Step 340/6780, Loss: 0.6027, Accuracy: 0.6160\n",
      "Step 350/6780, Loss: 1.1647, Accuracy: 0.6187\n",
      "Step 360/6780, Loss: 0.5030, Accuracy: 0.6200\n",
      "Step 370/6780, Loss: 0.4864, Accuracy: 0.6209\n",
      "Step 380/6780, Loss: 0.9689, Accuracy: 0.6234\n",
      "Step 390/6780, Loss: 1.0058, Accuracy: 0.6248\n",
      "Step 400/6780, Loss: 0.5559, Accuracy: 0.6267\n",
      "Step 410/6780, Loss: 0.7541, Accuracy: 0.6265\n",
      "Step 420/6780, Loss: 0.8774, Accuracy: 0.6269\n",
      "Step 430/6780, Loss: 0.7531, Accuracy: 0.6272\n",
      "Step 440/6780, Loss: 0.7957, Accuracy: 0.6293\n",
      "Step 450/6780, Loss: 0.5941, Accuracy: 0.6308\n",
      "Step 460/6780, Loss: 0.8201, Accuracy: 0.6321\n",
      "Step 470/6780, Loss: 0.6910, Accuracy: 0.6330\n",
      "Step 480/6780, Loss: 0.8419, Accuracy: 0.6346\n",
      "Step 490/6780, Loss: 1.1241, Accuracy: 0.6337\n",
      "Step 500/6780, Loss: 1.1013, Accuracy: 0.6345\n",
      "Step 510/6780, Loss: 0.6661, Accuracy: 0.6343\n",
      "Step 520/6780, Loss: 0.7827, Accuracy: 0.6347\n",
      "Step 530/6780, Loss: 0.9119, Accuracy: 0.6357\n",
      "Step 540/6780, Loss: 0.6883, Accuracy: 0.6367\n",
      "Step 550/6780, Loss: 0.8740, Accuracy: 0.6361\n",
      "Step 560/6780, Loss: 0.6088, Accuracy: 0.6363\n",
      "Step 570/6780, Loss: 0.8341, Accuracy: 0.6365\n",
      "Step 580/6780, Loss: 0.4527, Accuracy: 0.6372\n",
      "Step 590/6780, Loss: 0.8450, Accuracy: 0.6380\n",
      "Step 600/6780, Loss: 1.6144, Accuracy: 0.6376\n",
      "Step 610/6780, Loss: 0.5104, Accuracy: 0.6367\n",
      "Step 620/6780, Loss: 0.8717, Accuracy: 0.6370\n",
      "Step 630/6780, Loss: 0.8661, Accuracy: 0.6372\n",
      "Step 640/6780, Loss: 0.4683, Accuracy: 0.6382\n",
      "Step 650/6780, Loss: 0.8956, Accuracy: 0.6388\n",
      "Step 660/6780, Loss: 0.5310, Accuracy: 0.6396\n",
      "Step 670/6780, Loss: 0.6637, Accuracy: 0.6410\n",
      "Step 680/6780, Loss: 0.4838, Accuracy: 0.6411\n",
      "Step 690/6780, Loss: 0.7589, Accuracy: 0.6418\n",
      "Step 700/6780, Loss: 0.4285, Accuracy: 0.6429\n",
      "Step 710/6780, Loss: 0.5648, Accuracy: 0.6431\n",
      "Step 720/6780, Loss: 0.5181, Accuracy: 0.6445\n",
      "Step 730/6780, Loss: 0.5160, Accuracy: 0.6456\n",
      "Step 740/6780, Loss: 0.8192, Accuracy: 0.6469\n",
      "Step 750/6780, Loss: 0.5963, Accuracy: 0.6481\n",
      "Step 760/6780, Loss: 1.7337, Accuracy: 0.6488\n",
      "Step 770/6780, Loss: 0.8695, Accuracy: 0.6500\n",
      "Step 780/6780, Loss: 0.7332, Accuracy: 0.6502\n",
      "Step 790/6780, Loss: 0.5536, Accuracy: 0.6504\n",
      "Step 800/6780, Loss: 0.5119, Accuracy: 0.6513\n",
      "Step 810/6780, Loss: 1.1256, Accuracy: 0.6513\n",
      "Step 820/6780, Loss: 0.5059, Accuracy: 0.6511\n",
      "Step 830/6780, Loss: 0.3979, Accuracy: 0.6525\n",
      "Step 840/6780, Loss: 1.2083, Accuracy: 0.6525\n",
      "Step 850/6780, Loss: 0.9337, Accuracy: 0.6526\n",
      "Step 860/6780, Loss: 0.7004, Accuracy: 0.6531\n",
      "Step 870/6780, Loss: 0.6539, Accuracy: 0.6529\n",
      "Step 880/6780, Loss: 1.0733, Accuracy: 0.6528\n",
      "Step 890/6780, Loss: 0.7206, Accuracy: 0.6539\n",
      "Step 900/6780, Loss: 0.5119, Accuracy: 0.6550\n",
      "Step 910/6780, Loss: 0.8214, Accuracy: 0.6553\n",
      "Step 920/6780, Loss: 0.3554, Accuracy: 0.6562\n",
      "Step 930/6780, Loss: 0.4846, Accuracy: 0.6567\n",
      "Step 940/6780, Loss: 0.5554, Accuracy: 0.6570\n",
      "Step 950/6780, Loss: 0.4155, Accuracy: 0.6580\n",
      "Step 960/6780, Loss: 0.8420, Accuracy: 0.6580\n",
      "Step 970/6780, Loss: 0.4804, Accuracy: 0.6581\n",
      "Step 980/6780, Loss: 1.0026, Accuracy: 0.6579\n",
      "Step 990/6780, Loss: 0.6504, Accuracy: 0.6580\n",
      "Step 1000/6780, Loss: 0.6357, Accuracy: 0.6584\n",
      "Step 1010/6780, Loss: 0.4303, Accuracy: 0.6588\n",
      "Step 1020/6780, Loss: 0.6789, Accuracy: 0.6594\n",
      "Step 1030/6780, Loss: 0.6948, Accuracy: 0.6603\n",
      "Step 1040/6780, Loss: 1.1096, Accuracy: 0.6605\n",
      "Step 1050/6780, Loss: 0.8118, Accuracy: 0.6604\n",
      "Step 1060/6780, Loss: 0.6862, Accuracy: 0.6608\n",
      "Step 1070/6780, Loss: 0.4527, Accuracy: 0.6616\n",
      "Step 1080/6780, Loss: 0.7543, Accuracy: 0.6615\n",
      "Step 1090/6780, Loss: 0.5379, Accuracy: 0.6622\n",
      "Step 1100/6780, Loss: 0.3757, Accuracy: 0.6628\n",
      "Step 1110/6780, Loss: 0.4828, Accuracy: 0.6635\n",
      "Step 1120/6780, Loss: 0.6746, Accuracy: 0.6643\n",
      "Step 1130/6780, Loss: 0.6056, Accuracy: 0.6647\n",
      "Step 1140/6780, Loss: 0.8792, Accuracy: 0.6652\n",
      "Step 1150/6780, Loss: 1.2505, Accuracy: 0.6653\n",
      "Step 1160/6780, Loss: 0.8896, Accuracy: 0.6654\n",
      "Step 1170/6780, Loss: 0.4465, Accuracy: 0.6660\n",
      "Step 1180/6780, Loss: 0.3520, Accuracy: 0.6668\n",
      "Step 1190/6780, Loss: 0.7275, Accuracy: 0.6675\n",
      "Step 1200/6780, Loss: 0.5435, Accuracy: 0.6678\n",
      "Step 1210/6780, Loss: 0.7400, Accuracy: 0.6683\n",
      "Step 1220/6780, Loss: 0.3227, Accuracy: 0.6691\n",
      "Step 1230/6780, Loss: 0.2214, Accuracy: 0.6695\n",
      "Step 1240/6780, Loss: 0.6194, Accuracy: 0.6701\n",
      "Step 1250/6780, Loss: 0.6214, Accuracy: 0.6705\n",
      "Step 1260/6780, Loss: 0.8331, Accuracy: 0.6706\n",
      "Step 1270/6780, Loss: 0.4214, Accuracy: 0.6714\n",
      "Step 1280/6780, Loss: 0.5006, Accuracy: 0.6720\n",
      "Step 1290/6780, Loss: 0.7713, Accuracy: 0.6723\n",
      "Step 1300/6780, Loss: 0.6074, Accuracy: 0.6728\n",
      "Step 1310/6780, Loss: 0.2537, Accuracy: 0.6737\n",
      "Step 1320/6780, Loss: 0.6223, Accuracy: 0.6743\n",
      "Step 1330/6780, Loss: 0.4177, Accuracy: 0.6747\n",
      "Step 1340/6780, Loss: 0.4115, Accuracy: 0.6753\n",
      "Step 1350/6780, Loss: 0.5772, Accuracy: 0.6759\n",
      "Step 1360/6780, Loss: 0.5593, Accuracy: 0.6760\n",
      "Step 1370/6780, Loss: 0.5808, Accuracy: 0.6761\n",
      "Step 1380/6780, Loss: 0.7079, Accuracy: 0.6769\n",
      "Step 1390/6780, Loss: 0.4470, Accuracy: 0.6774\n",
      "Step 1400/6780, Loss: 0.6610, Accuracy: 0.6777\n",
      "Step 1410/6780, Loss: 0.7010, Accuracy: 0.6781\n",
      "Step 1420/6780, Loss: 1.0426, Accuracy: 0.6780\n",
      "Step 1430/6780, Loss: 0.9911, Accuracy: 0.6783\n",
      "Step 1440/6780, Loss: 0.6067, Accuracy: 0.6788\n",
      "Step 1450/6780, Loss: 0.4772, Accuracy: 0.6792\n",
      "Step 1460/6780, Loss: 0.3809, Accuracy: 0.6797\n",
      "Step 1470/6780, Loss: 0.7093, Accuracy: 0.6799\n",
      "Step 1480/6780, Loss: 0.3426, Accuracy: 0.6807\n",
      "Step 1490/6780, Loss: 0.6692, Accuracy: 0.6807\n",
      "Step 1500/6780, Loss: 0.6687, Accuracy: 0.6813\n",
      "Step 1510/6780, Loss: 0.4229, Accuracy: 0.6820\n",
      "Step 1520/6780, Loss: 0.4279, Accuracy: 0.6822\n",
      "Step 1530/6780, Loss: 0.4955, Accuracy: 0.6827\n",
      "Step 1540/6780, Loss: 0.6716, Accuracy: 0.6832\n",
      "Step 1550/6780, Loss: 0.4333, Accuracy: 0.6835\n",
      "Step 1560/6780, Loss: 0.5215, Accuracy: 0.6839\n",
      "Step 1570/6780, Loss: 0.2936, Accuracy: 0.6844\n",
      "Step 1580/6780, Loss: 0.3815, Accuracy: 0.6849\n",
      "Step 1590/6780, Loss: 0.3944, Accuracy: 0.6854\n",
      "Step 1600/6780, Loss: 0.6736, Accuracy: 0.6859\n",
      "Step 1610/6780, Loss: 0.6299, Accuracy: 0.6862\n",
      "Step 1620/6780, Loss: 0.3524, Accuracy: 0.6863\n",
      "Step 1630/6780, Loss: 1.3342, Accuracy: 0.6867\n",
      "Step 1640/6780, Loss: 0.3744, Accuracy: 0.6867\n",
      "Step 1650/6780, Loss: 0.3627, Accuracy: 0.6872\n",
      "Step 1660/6780, Loss: 0.3902, Accuracy: 0.6877\n",
      "Step 1670/6780, Loss: 0.4629, Accuracy: 0.6882\n",
      "Step 1680/6780, Loss: 0.7200, Accuracy: 0.6881\n",
      "Step 1690/6780, Loss: 0.7400, Accuracy: 0.6885\n",
      "Step 1700/6780, Loss: 0.6555, Accuracy: 0.6888\n",
      "Step 1710/6780, Loss: 0.1862, Accuracy: 0.6895\n",
      "Step 1720/6780, Loss: 0.3399, Accuracy: 0.6897\n",
      "Step 1730/6780, Loss: 0.6889, Accuracy: 0.6903\n",
      "Step 1740/6780, Loss: 0.4259, Accuracy: 0.6904\n",
      "Step 1750/6780, Loss: 0.4115, Accuracy: 0.6911\n",
      "Step 1760/6780, Loss: 0.7329, Accuracy: 0.6910\n",
      "Step 1770/6780, Loss: 0.4228, Accuracy: 0.6911\n",
      "Step 1780/6780, Loss: 0.3733, Accuracy: 0.6914\n",
      "Step 1790/6780, Loss: 1.2038, Accuracy: 0.6915\n",
      "Step 1800/6780, Loss: 0.4977, Accuracy: 0.6915\n",
      "Step 1810/6780, Loss: 0.4176, Accuracy: 0.6918\n",
      "Step 1820/6780, Loss: 0.6336, Accuracy: 0.6921\n",
      "Step 1830/6780, Loss: 0.5477, Accuracy: 0.6924\n",
      "Step 1840/6780, Loss: 0.3213, Accuracy: 0.6928\n",
      "Step 1850/6780, Loss: 0.5287, Accuracy: 0.6930\n",
      "Step 1860/6780, Loss: 0.6079, Accuracy: 0.6930\n",
      "Step 1870/6780, Loss: 0.5920, Accuracy: 0.6930\n",
      "Step 1880/6780, Loss: 0.8378, Accuracy: 0.6934\n",
      "Step 1890/6780, Loss: 0.6952, Accuracy: 0.6935\n",
      "Step 1900/6780, Loss: 1.0754, Accuracy: 0.6937\n",
      "Step 1910/6780, Loss: 0.2791, Accuracy: 0.6943\n",
      "Step 1920/6780, Loss: 0.5625, Accuracy: 0.6947\n",
      "Step 1930/6780, Loss: 0.6300, Accuracy: 0.6947\n",
      "Step 1940/6780, Loss: 0.5665, Accuracy: 0.6951\n",
      "Step 1950/6780, Loss: 0.6414, Accuracy: 0.6952\n",
      "Step 1960/6780, Loss: 0.5706, Accuracy: 0.6951\n",
      "Step 1970/6780, Loss: 0.5729, Accuracy: 0.6951\n",
      "Step 1980/6780, Loss: 0.5170, Accuracy: 0.6952\n",
      "Step 1990/6780, Loss: 0.4526, Accuracy: 0.6952\n",
      "Step 2000/6780, Loss: 1.0443, Accuracy: 0.6956\n",
      "Step 2010/6780, Loss: 0.9172, Accuracy: 0.6960\n",
      "Step 2020/6780, Loss: 0.3807, Accuracy: 0.6964\n",
      "Step 2030/6780, Loss: 0.7547, Accuracy: 0.6967\n",
      "Step 2040/6780, Loss: 0.5677, Accuracy: 0.6967\n",
      "Step 2050/6780, Loss: 0.7242, Accuracy: 0.6965\n",
      "Step 2060/6780, Loss: 0.7149, Accuracy: 0.6970\n",
      "Step 2070/6780, Loss: 0.3391, Accuracy: 0.6974\n",
      "Step 2080/6780, Loss: 0.3124, Accuracy: 0.6976\n",
      "Step 2090/6780, Loss: 0.5707, Accuracy: 0.6977\n",
      "Step 2100/6780, Loss: 0.7094, Accuracy: 0.6979\n",
      "Step 2110/6780, Loss: 0.7551, Accuracy: 0.6983\n",
      "Step 2120/6780, Loss: 0.8691, Accuracy: 0.6985\n",
      "Step 2130/6780, Loss: 1.1227, Accuracy: 0.6989\n",
      "Step 2140/6780, Loss: 0.3800, Accuracy: 0.6992\n",
      "Step 2150/6780, Loss: 0.6863, Accuracy: 0.6993\n",
      "Step 2160/6780, Loss: 0.4440, Accuracy: 0.6997\n",
      "Step 2170/6780, Loss: 0.6010, Accuracy: 0.7003\n",
      "Step 2180/6780, Loss: 0.5974, Accuracy: 0.7003\n",
      "Step 2190/6780, Loss: 0.8655, Accuracy: 0.7007\n",
      "Step 2200/6780, Loss: 0.7599, Accuracy: 0.7013\n",
      "Step 2210/6780, Loss: 0.4895, Accuracy: 0.7018\n",
      "Step 2220/6780, Loss: 0.6632, Accuracy: 0.7021\n",
      "Step 2230/6780, Loss: 0.5068, Accuracy: 0.7024\n",
      "Step 2240/6780, Loss: 0.8173, Accuracy: 0.7027\n",
      "Step 2250/6780, Loss: 0.7081, Accuracy: 0.7029\n",
      "Step 2260/6780, Loss: 0.5584, Accuracy: 0.7033\n",
      "Step 2270/6780, Loss: 0.4713, Accuracy: 0.7037\n",
      "Step 2280/6780, Loss: 0.5182, Accuracy: 0.7037\n",
      "Step 2290/6780, Loss: 0.5551, Accuracy: 0.7039\n",
      "Step 2300/6780, Loss: 0.3457, Accuracy: 0.7043\n",
      "Step 2310/6780, Loss: 0.8286, Accuracy: 0.7044\n",
      "Step 2320/6780, Loss: 0.4557, Accuracy: 0.7045\n",
      "Step 2330/6780, Loss: 0.4571, Accuracy: 0.7047\n",
      "Step 2340/6780, Loss: 0.5380, Accuracy: 0.7050\n",
      "Step 2350/6780, Loss: 0.5354, Accuracy: 0.7049\n",
      "Step 2360/6780, Loss: 0.6647, Accuracy: 0.7050\n",
      "Step 2370/6780, Loss: 0.2611, Accuracy: 0.7054\n",
      "Step 2380/6780, Loss: 0.5185, Accuracy: 0.7055\n",
      "Step 2390/6780, Loss: 0.8160, Accuracy: 0.7057\n",
      "Step 2400/6780, Loss: 0.5130, Accuracy: 0.7059\n",
      "Step 2410/6780, Loss: 0.4739, Accuracy: 0.7064\n",
      "Step 2420/6780, Loss: 0.6213, Accuracy: 0.7067\n",
      "Step 2430/6780, Loss: 1.1226, Accuracy: 0.7069\n",
      "Step 2440/6780, Loss: 0.4362, Accuracy: 0.7070\n",
      "Step 2450/6780, Loss: 0.5373, Accuracy: 0.7069\n",
      "Step 2460/6780, Loss: 0.6018, Accuracy: 0.7068\n",
      "Step 2470/6780, Loss: 0.3114, Accuracy: 0.7073\n",
      "Step 2480/6780, Loss: 0.3891, Accuracy: 0.7073\n",
      "Step 2490/6780, Loss: 0.7487, Accuracy: 0.7074\n",
      "Step 2500/6780, Loss: 0.6422, Accuracy: 0.7074\n",
      "Step 2510/6780, Loss: 0.2668, Accuracy: 0.7076\n",
      "Step 2520/6780, Loss: 0.7502, Accuracy: 0.7079\n",
      "Step 2530/6780, Loss: 0.4411, Accuracy: 0.7079\n",
      "Step 2540/6780, Loss: 0.3318, Accuracy: 0.7082\n",
      "Step 2550/6780, Loss: 0.4021, Accuracy: 0.7082\n",
      "Step 2560/6780, Loss: 0.4218, Accuracy: 0.7083\n",
      "Step 2570/6780, Loss: 0.4534, Accuracy: 0.7085\n",
      "Step 2580/6780, Loss: 0.5304, Accuracy: 0.7087\n",
      "Step 2590/6780, Loss: 0.6859, Accuracy: 0.7090\n",
      "Step 2600/6780, Loss: 0.5674, Accuracy: 0.7089\n",
      "Step 2610/6780, Loss: 0.5051, Accuracy: 0.7092\n",
      "Step 2620/6780, Loss: 0.9079, Accuracy: 0.7093\n",
      "Step 2630/6780, Loss: 0.6487, Accuracy: 0.7094\n",
      "Step 2640/6780, Loss: 0.6308, Accuracy: 0.7095\n",
      "Step 2650/6780, Loss: 0.2528, Accuracy: 0.7099\n",
      "Step 2660/6780, Loss: 0.3070, Accuracy: 0.7101\n",
      "Step 2670/6780, Loss: 0.4656, Accuracy: 0.7102\n",
      "Step 2680/6780, Loss: 0.7191, Accuracy: 0.7102\n",
      "Step 2690/6780, Loss: 0.5201, Accuracy: 0.7105\n",
      "Step 2700/6780, Loss: 1.3178, Accuracy: 0.7106\n",
      "Step 2710/6780, Loss: 0.4674, Accuracy: 0.7109\n",
      "Step 2720/6780, Loss: 0.6923, Accuracy: 0.7110\n",
      "Step 2730/6780, Loss: 0.3285, Accuracy: 0.7111\n",
      "Step 2740/6780, Loss: 0.7881, Accuracy: 0.7114\n",
      "Step 2750/6780, Loss: 0.4151, Accuracy: 0.7118\n",
      "Step 2760/6780, Loss: 0.5405, Accuracy: 0.7121\n",
      "Step 2770/6780, Loss: 0.5002, Accuracy: 0.7123\n",
      "Step 2780/6780, Loss: 0.5445, Accuracy: 0.7126\n",
      "Step 2790/6780, Loss: 0.2939, Accuracy: 0.7128\n",
      "Step 2800/6780, Loss: 0.4227, Accuracy: 0.7130\n",
      "Step 2810/6780, Loss: 0.4274, Accuracy: 0.7133\n",
      "Step 2820/6780, Loss: 0.4999, Accuracy: 0.7133\n",
      "Step 2830/6780, Loss: 0.4844, Accuracy: 0.7135\n",
      "Step 2840/6780, Loss: 0.4760, Accuracy: 0.7137\n",
      "Step 2850/6780, Loss: 0.5915, Accuracy: 0.7139\n",
      "Step 2860/6780, Loss: 0.4352, Accuracy: 0.7141\n",
      "Step 2870/6780, Loss: 0.4665, Accuracy: 0.7142\n",
      "Step 2880/6780, Loss: 0.7932, Accuracy: 0.7143\n",
      "Step 2890/6780, Loss: 0.5795, Accuracy: 0.7145\n",
      "Step 2900/6780, Loss: 0.5923, Accuracy: 0.7146\n",
      "Step 2910/6780, Loss: 0.3389, Accuracy: 0.7150\n",
      "Step 2920/6780, Loss: 0.8006, Accuracy: 0.7152\n",
      "Step 2930/6780, Loss: 0.5228, Accuracy: 0.7154\n",
      "Step 2940/6780, Loss: 0.5096, Accuracy: 0.7157\n",
      "Step 2950/6780, Loss: 0.5709, Accuracy: 0.7158\n",
      "Step 2960/6780, Loss: 0.7330, Accuracy: 0.7158\n",
      "Step 2970/6780, Loss: 0.4876, Accuracy: 0.7158\n",
      "Step 2980/6780, Loss: 0.4300, Accuracy: 0.7159\n",
      "Step 2990/6780, Loss: 0.4117, Accuracy: 0.7162\n",
      "Step 3000/6780, Loss: 0.7485, Accuracy: 0.7165\n",
      "Step 3010/6780, Loss: 1.2965, Accuracy: 0.7164\n",
      "Step 3020/6780, Loss: 0.3347, Accuracy: 0.7166\n",
      "Step 3030/6780, Loss: 0.6578, Accuracy: 0.7167\n",
      "Step 3040/6780, Loss: 0.6374, Accuracy: 0.7169\n",
      "Step 3050/6780, Loss: 0.8520, Accuracy: 0.7172\n",
      "Step 3060/6780, Loss: 1.1291, Accuracy: 0.7172\n",
      "Step 3070/6780, Loss: 0.5158, Accuracy: 0.7174\n",
      "Step 3080/6780, Loss: 0.9051, Accuracy: 0.7174\n",
      "Step 3090/6780, Loss: 0.4020, Accuracy: 0.7176\n",
      "Step 3100/6780, Loss: 0.3847, Accuracy: 0.7180\n",
      "Step 3110/6780, Loss: 0.4898, Accuracy: 0.7182\n",
      "Step 3120/6780, Loss: 0.4186, Accuracy: 0.7184\n",
      "Step 3130/6780, Loss: 0.2058, Accuracy: 0.7184\n",
      "Step 3140/6780, Loss: 0.5142, Accuracy: 0.7186\n",
      "Step 3150/6780, Loss: 0.5822, Accuracy: 0.7189\n",
      "Step 3160/6780, Loss: 0.7464, Accuracy: 0.7190\n",
      "Step 3170/6780, Loss: 0.6092, Accuracy: 0.7192\n",
      "Step 3180/6780, Loss: 0.3570, Accuracy: 0.7193\n",
      "Step 3190/6780, Loss: 0.5500, Accuracy: 0.7195\n",
      "Step 3200/6780, Loss: 0.6556, Accuracy: 0.7197\n",
      "Step 3210/6780, Loss: 0.9332, Accuracy: 0.7199\n",
      "Step 3220/6780, Loss: 0.7873, Accuracy: 0.7198\n",
      "Step 3230/6780, Loss: 0.5251, Accuracy: 0.7199\n",
      "Step 3240/6780, Loss: 0.8720, Accuracy: 0.7201\n",
      "Step 3250/6780, Loss: 0.6685, Accuracy: 0.7202\n",
      "Step 3260/6780, Loss: 0.5563, Accuracy: 0.7203\n",
      "Step 3270/6780, Loss: 0.2755, Accuracy: 0.7204\n",
      "Step 3280/6780, Loss: 0.4060, Accuracy: 0.7206\n",
      "Step 3290/6780, Loss: 0.3819, Accuracy: 0.7207\n",
      "Step 3300/6780, Loss: 0.4097, Accuracy: 0.7208\n",
      "Step 3310/6780, Loss: 0.5168, Accuracy: 0.7209\n",
      "Step 3320/6780, Loss: 0.7164, Accuracy: 0.7211\n",
      "Step 3330/6780, Loss: 0.3334, Accuracy: 0.7212\n",
      "Step 3340/6780, Loss: 0.3782, Accuracy: 0.7215\n",
      "Step 3350/6780, Loss: 0.4509, Accuracy: 0.7216\n",
      "Step 3360/6780, Loss: 0.4183, Accuracy: 0.7219\n",
      "Step 3370/6780, Loss: 0.4158, Accuracy: 0.7221\n",
      "Step 3380/6780, Loss: 0.6729, Accuracy: 0.7223\n",
      "Step 3390/6780, Loss: 0.5192, Accuracy: 0.7224\n",
      "Step 3400/6780, Loss: 0.4892, Accuracy: 0.7226\n",
      "Step 3410/6780, Loss: 0.4267, Accuracy: 0.7228\n",
      "Step 3420/6780, Loss: 0.4590, Accuracy: 0.7229\n",
      "Step 3430/6780, Loss: 0.7310, Accuracy: 0.7231\n",
      "Step 3440/6780, Loss: 0.5064, Accuracy: 0.7232\n",
      "Step 3450/6780, Loss: 0.4225, Accuracy: 0.7234\n",
      "Step 3460/6780, Loss: 0.3808, Accuracy: 0.7235\n",
      "Step 3470/6780, Loss: 0.4323, Accuracy: 0.7238\n",
      "Step 3480/6780, Loss: 1.0521, Accuracy: 0.7240\n",
      "Step 3490/6780, Loss: 0.5200, Accuracy: 0.7241\n",
      "Step 3500/6780, Loss: 0.1713, Accuracy: 0.7244\n",
      "Step 3510/6780, Loss: 0.3326, Accuracy: 0.7247\n",
      "Step 3520/6780, Loss: 0.5477, Accuracy: 0.7247\n",
      "Step 3530/6780, Loss: 0.6033, Accuracy: 0.7249\n",
      "Step 3540/6780, Loss: 0.4950, Accuracy: 0.7250\n",
      "Step 3550/6780, Loss: 0.6068, Accuracy: 0.7252\n",
      "Step 3560/6780, Loss: 0.5843, Accuracy: 0.7255\n",
      "Step 3570/6780, Loss: 1.0089, Accuracy: 0.7257\n",
      "Step 3580/6780, Loss: 0.5095, Accuracy: 0.7259\n",
      "Step 3590/6780, Loss: 0.8503, Accuracy: 0.7259\n",
      "Step 3600/6780, Loss: 0.3681, Accuracy: 0.7261\n",
      "Step 3610/6780, Loss: 0.2457, Accuracy: 0.7263\n",
      "Step 3620/6780, Loss: 0.2469, Accuracy: 0.7265\n",
      "Step 3630/6780, Loss: 0.4059, Accuracy: 0.7266\n",
      "Step 3640/6780, Loss: 0.3548, Accuracy: 0.7267\n",
      "Step 3650/6780, Loss: 0.4585, Accuracy: 0.7269\n",
      "Step 3660/6780, Loss: 0.4962, Accuracy: 0.7271\n",
      "Step 3670/6780, Loss: 0.4205, Accuracy: 0.7273\n",
      "Step 3680/6780, Loss: 0.3539, Accuracy: 0.7276\n",
      "Step 3690/6780, Loss: 0.6465, Accuracy: 0.7277\n",
      "Step 3700/6780, Loss: 0.8113, Accuracy: 0.7278\n",
      "Step 3710/6780, Loss: 0.4865, Accuracy: 0.7280\n",
      "Step 3720/6780, Loss: 0.4752, Accuracy: 0.7281\n",
      "Step 3730/6780, Loss: 0.4587, Accuracy: 0.7282\n",
      "Step 3740/6780, Loss: 0.4331, Accuracy: 0.7283\n",
      "Step 3750/6780, Loss: 0.2732, Accuracy: 0.7284\n",
      "Step 3760/6780, Loss: 0.4952, Accuracy: 0.7286\n",
      "Step 3770/6780, Loss: 0.4921, Accuracy: 0.7288\n",
      "Step 3780/6780, Loss: 0.7384, Accuracy: 0.7289\n",
      "Step 3790/6780, Loss: 0.5671, Accuracy: 0.7291\n",
      "Step 3800/6780, Loss: 0.6195, Accuracy: 0.7293\n",
      "Step 3810/6780, Loss: 0.8295, Accuracy: 0.7294\n",
      "Step 3820/6780, Loss: 0.6332, Accuracy: 0.7296\n",
      "Step 3830/6780, Loss: 0.3740, Accuracy: 0.7297\n",
      "Step 3840/6780, Loss: 0.4343, Accuracy: 0.7300\n",
      "Step 3850/6780, Loss: 0.9349, Accuracy: 0.7302\n",
      "Step 3860/6780, Loss: 0.3135, Accuracy: 0.7303\n",
      "Step 3870/6780, Loss: 0.6383, Accuracy: 0.7305\n",
      "Step 3880/6780, Loss: 0.4953, Accuracy: 0.7306\n",
      "Step 3890/6780, Loss: 0.6891, Accuracy: 0.7308\n",
      "Step 3900/6780, Loss: 0.3107, Accuracy: 0.7310\n",
      "Step 3910/6780, Loss: 0.6327, Accuracy: 0.7311\n",
      "Step 3920/6780, Loss: 0.3730, Accuracy: 0.7312\n",
      "Step 3930/6780, Loss: 0.6215, Accuracy: 0.7312\n",
      "Step 3940/6780, Loss: 0.3756, Accuracy: 0.7313\n",
      "Step 3950/6780, Loss: 0.2609, Accuracy: 0.7316\n",
      "Step 3960/6780, Loss: 0.7165, Accuracy: 0.7318\n",
      "Step 3970/6780, Loss: 0.6579, Accuracy: 0.7321\n",
      "Step 3980/6780, Loss: 0.4836, Accuracy: 0.7322\n",
      "Step 3990/6780, Loss: 0.4393, Accuracy: 0.7323\n",
      "Step 4000/6780, Loss: 0.6861, Accuracy: 0.7325\n",
      "Step 4010/6780, Loss: 0.4774, Accuracy: 0.7326\n",
      "Step 4020/6780, Loss: 0.6506, Accuracy: 0.7326\n",
      "Step 4030/6780, Loss: 0.6876, Accuracy: 0.7328\n",
      "Step 4040/6780, Loss: 0.5280, Accuracy: 0.7328\n",
      "Step 4050/6780, Loss: 0.8291, Accuracy: 0.7330\n",
      "Step 4060/6780, Loss: 0.5282, Accuracy: 0.7331\n",
      "Step 4070/6780, Loss: 0.5299, Accuracy: 0.7330\n",
      "Step 4080/6780, Loss: 0.4773, Accuracy: 0.7331\n",
      "Step 4090/6780, Loss: 0.4567, Accuracy: 0.7333\n",
      "Step 4100/6780, Loss: 0.3598, Accuracy: 0.7335\n",
      "Step 4110/6780, Loss: 0.4293, Accuracy: 0.7337\n",
      "Step 4120/6780, Loss: 0.7037, Accuracy: 0.7339\n",
      "Step 4130/6780, Loss: 0.1816, Accuracy: 0.7341\n",
      "Step 4140/6780, Loss: 0.4304, Accuracy: 0.7342\n",
      "Step 4150/6780, Loss: 0.4008, Accuracy: 0.7344\n",
      "Step 4160/6780, Loss: 0.3108, Accuracy: 0.7344\n",
      "Step 4170/6780, Loss: 0.3339, Accuracy: 0.7346\n",
      "Step 4180/6780, Loss: 0.3774, Accuracy: 0.7347\n",
      "Step 4190/6780, Loss: 0.6466, Accuracy: 0.7348\n",
      "Step 4200/6780, Loss: 0.2711, Accuracy: 0.7350\n",
      "Step 4210/6780, Loss: 0.6539, Accuracy: 0.7350\n",
      "Step 4220/6780, Loss: 0.4208, Accuracy: 0.7351\n",
      "Step 4230/6780, Loss: 0.7759, Accuracy: 0.7352\n",
      "Step 4240/6780, Loss: 0.3350, Accuracy: 0.7353\n",
      "Step 4250/6780, Loss: 0.4883, Accuracy: 0.7356\n",
      "Step 4260/6780, Loss: 0.4774, Accuracy: 0.7357\n",
      "Step 4270/6780, Loss: 0.3029, Accuracy: 0.7360\n",
      "Step 4280/6780, Loss: 0.3022, Accuracy: 0.7362\n",
      "Step 4290/6780, Loss: 0.4150, Accuracy: 0.7364\n",
      "Step 4300/6780, Loss: 0.6812, Accuracy: 0.7366\n",
      "Step 4310/6780, Loss: 0.8662, Accuracy: 0.7367\n",
      "Step 4320/6780, Loss: 0.4270, Accuracy: 0.7367\n",
      "Step 4330/6780, Loss: 0.2725, Accuracy: 0.7368\n",
      "Step 4340/6780, Loss: 0.4154, Accuracy: 0.7370\n",
      "Step 4350/6780, Loss: 0.6077, Accuracy: 0.7371\n",
      "Step 4360/6780, Loss: 0.6975, Accuracy: 0.7372\n",
      "Step 4370/6780, Loss: 0.5967, Accuracy: 0.7373\n",
      "Step 4380/6780, Loss: 0.3511, Accuracy: 0.7374\n",
      "Step 4390/6780, Loss: 0.7004, Accuracy: 0.7376\n",
      "Step 4400/6780, Loss: 0.7111, Accuracy: 0.7377\n",
      "Step 4410/6780, Loss: 0.2669, Accuracy: 0.7379\n",
      "Step 4420/6780, Loss: 0.2778, Accuracy: 0.7381\n",
      "Step 4430/6780, Loss: 0.3831, Accuracy: 0.7382\n",
      "Step 4440/6780, Loss: 0.5159, Accuracy: 0.7383\n",
      "Step 4450/6780, Loss: 0.4118, Accuracy: 0.7385\n",
      "Step 4460/6780, Loss: 0.3855, Accuracy: 0.7387\n",
      "Step 4470/6780, Loss: 0.3131, Accuracy: 0.7387\n",
      "Step 4480/6780, Loss: 0.7369, Accuracy: 0.7387\n",
      "Step 4490/6780, Loss: 0.4441, Accuracy: 0.7389\n",
      "Step 4500/6780, Loss: 0.4715, Accuracy: 0.7390\n",
      "Step 4510/6780, Loss: 0.6566, Accuracy: 0.7390\n",
      "Step 4520/6780, Loss: 0.5886, Accuracy: 0.7392\n",
      "Step 4530/6780, Loss: 0.6808, Accuracy: 0.7392\n",
      "Step 4540/6780, Loss: 0.5215, Accuracy: 0.7393\n",
      "Step 4550/6780, Loss: 0.4551, Accuracy: 0.7394\n",
      "Step 4560/6780, Loss: 0.8536, Accuracy: 0.7394\n",
      "Step 4570/6780, Loss: 0.3412, Accuracy: 0.7395\n",
      "Step 4580/6780, Loss: 0.9384, Accuracy: 0.7394\n",
      "Step 4590/6780, Loss: 0.6256, Accuracy: 0.7395\n",
      "Step 4600/6780, Loss: 0.3441, Accuracy: 0.7396\n",
      "Step 4610/6780, Loss: 0.5927, Accuracy: 0.7397\n",
      "Step 4620/6780, Loss: 0.2961, Accuracy: 0.7398\n",
      "Step 4630/6780, Loss: 0.5150, Accuracy: 0.7398\n",
      "Step 4640/6780, Loss: 0.3600, Accuracy: 0.7400\n",
      "Step 4650/6780, Loss: 0.4128, Accuracy: 0.7401\n",
      "Step 4660/6780, Loss: 0.4731, Accuracy: 0.7404\n",
      "Step 4670/6780, Loss: 0.3057, Accuracy: 0.7405\n",
      "Step 4680/6780, Loss: 0.3777, Accuracy: 0.7406\n",
      "Step 4690/6780, Loss: 0.5116, Accuracy: 0.7406\n",
      "Step 4700/6780, Loss: 0.5878, Accuracy: 0.7407\n",
      "Step 4710/6780, Loss: 0.2628, Accuracy: 0.7409\n",
      "Step 4720/6780, Loss: 0.6892, Accuracy: 0.7410\n",
      "Step 4730/6780, Loss: 0.4564, Accuracy: 0.7410\n",
      "Step 4740/6780, Loss: 0.8119, Accuracy: 0.7411\n",
      "Step 4750/6780, Loss: 0.3256, Accuracy: 0.7412\n",
      "Step 4760/6780, Loss: 0.7253, Accuracy: 0.7414\n",
      "Step 4770/6780, Loss: 0.5501, Accuracy: 0.7415\n",
      "Step 4780/6780, Loss: 0.3775, Accuracy: 0.7415\n",
      "Step 4790/6780, Loss: 0.5280, Accuracy: 0.7415\n",
      "Step 4800/6780, Loss: 0.2858, Accuracy: 0.7416\n",
      "Step 4810/6780, Loss: 0.6005, Accuracy: 0.7418\n",
      "Step 4820/6780, Loss: 0.3777, Accuracy: 0.7419\n",
      "Step 4830/6780, Loss: 0.3984, Accuracy: 0.7421\n",
      "Step 4840/6780, Loss: 0.5075, Accuracy: 0.7422\n",
      "Step 4850/6780, Loss: 0.4353, Accuracy: 0.7423\n",
      "Step 4860/6780, Loss: 0.2003, Accuracy: 0.7425\n",
      "Step 4870/6780, Loss: 0.4900, Accuracy: 0.7427\n",
      "Step 4880/6780, Loss: 0.4631, Accuracy: 0.7428\n",
      "Step 4890/6780, Loss: 0.6238, Accuracy: 0.7429\n",
      "Step 4900/6780, Loss: 0.3051, Accuracy: 0.7430\n",
      "Step 4910/6780, Loss: 0.3560, Accuracy: 0.7431\n",
      "Step 4920/6780, Loss: 0.3158, Accuracy: 0.7432\n",
      "Step 4930/6780, Loss: 0.1708, Accuracy: 0.7434\n",
      "Step 4940/6780, Loss: 0.3443, Accuracy: 0.7435\n",
      "Step 4950/6780, Loss: 0.3696, Accuracy: 0.7436\n",
      "Step 4960/6780, Loss: 0.2238, Accuracy: 0.7436\n",
      "Step 4970/6780, Loss: 0.7078, Accuracy: 0.7438\n",
      "Step 4980/6780, Loss: 0.3997, Accuracy: 0.7440\n",
      "Step 4990/6780, Loss: 0.4610, Accuracy: 0.7441\n",
      "Step 5000/6780, Loss: 0.4028, Accuracy: 0.7442\n",
      "Step 5010/6780, Loss: 0.2675, Accuracy: 0.7443\n",
      "Step 5020/6780, Loss: 0.4993, Accuracy: 0.7445\n",
      "Step 5030/6780, Loss: 0.3245, Accuracy: 0.7447\n",
      "Step 5040/6780, Loss: 0.2473, Accuracy: 0.7448\n",
      "Step 5050/6780, Loss: 0.3996, Accuracy: 0.7449\n",
      "Step 5060/6780, Loss: 0.7020, Accuracy: 0.7450\n",
      "Step 5070/6780, Loss: 0.6984, Accuracy: 0.7451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 251\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Two-stage training with gradient accumulation\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 1: Feature extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 251\u001b[0m \u001b[43mtrain_with_gradient_accumulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Fine-tuning\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage 2: Fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 185\u001b[0m, in \u001b[0;36mtrain_with_gradient_accumulation\u001b[1;34m(model, train_generator, validation_generator, steps_per_epoch, validation_steps, epochs, callbacks)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(accum_steps):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m         x_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m         train_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_generator)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\preprocessing\\image.py:156\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\preprocessing\\image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\preprocessing\\image.py:370\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    368\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[1;32m--> 370\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     x \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mimg_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\utils\\image_utils.py:480\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    478\u001b[0m             img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize(width_height_tuple, resample, box\u001b[38;5;241m=\u001b[39mcrop_box)\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m             img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth_height_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\PIL\\Image.py:2336\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2333\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(size, resample, box)\n\u001b[0;32m   2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m-> 2336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample \u001b[38;5;241m!=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST:\n\u001b[0;32m   2339\u001b[0m     factor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\py39\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, Dense, GlobalAveragePooling2D, MaxPooling2D,\n",
    "    BatchNormalization, Dropout, Concatenate, Add, LeakyReLU,\n",
    "    SeparableConv2D, Activation, AveragePooling2D,\n",
    "    GlobalMaxPooling2D, Reshape, Multiply\n",
    ")\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, BackupAndRestore\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 3\n",
    "EPOCHS = 75\n",
    "BASE_LR = 2e-4\n",
    "WEIGHT_DECAY = 2e-5\n",
    "\n",
    "# Enhanced GPU settings with mixed precision\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Using {len(gpus)} GPU(s) with mixed precision\")\n",
    "else:\n",
    "    print(\"Using CPU with mixed precision\")\n",
    "\n",
    "# Enhanced attention mechanism with spatial attention\n",
    "def cbam_block(input_tensor, ratio=8):\n",
    "    channel_avg = GlobalAveragePooling2D()(input_tensor)\n",
    "    channel_max = GlobalMaxPooling2D()(input_tensor)\n",
    "    channel = Concatenate()([channel_avg, channel_max])\n",
    "    channel = Reshape((1, 1, -1))(channel)\n",
    "    channel = Dense(input_tensor.shape[-1] // ratio, activation='relu')(channel)\n",
    "    channel = Dense(input_tensor.shape[-1], activation='sigmoid')(channel)\n",
    "    \n",
    "    channel_out = Multiply()([input_tensor, channel])\n",
    "    \n",
    "    spatial_avg = tf.reduce_mean(channel_out, axis=-1, keepdims=True)\n",
    "    spatial_max = tf.reduce_max(channel_out, axis=-1, keepdims=True)\n",
    "    spatial = Concatenate(axis=-1)([spatial_avg, spatial_max])\n",
    "    spatial = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(spatial)\n",
    "    \n",
    "    return Multiply()([channel_out, spatial])\n",
    "\n",
    "# Enhanced residual block\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    shortcut = x\n",
    "    \n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), strides=stride, padding='same')(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    y = SeparableConv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU(0.1)(y)\n",
    "    \n",
    "    y = SeparableConv2D(filters, kernel_size, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = cbam_block(y)\n",
    "    \n",
    "    output = Add()([shortcut, y])\n",
    "    return LeakyReLU(0.1)(output)\n",
    "\n",
    "# Simplified ensemble model\n",
    "def create_ensemble_model():\n",
    "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "    for layer in efficientnet.layers[-40:]:\n",
    "        layer.trainable = True\n",
    "    eff_features = efficientnet.output\n",
    "    \n",
    "    x = Conv2D(32, (7, 7), strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128)\n",
    "    \n",
    "    eff_avg = GlobalAveragePooling2D()(eff_features)\n",
    "    custom_avg = GlobalAveragePooling2D()(x)\n",
    "    combined = Concatenate()([eff_avg, custom_avg])\n",
    "    \n",
    "    combined = Dense(512, kernel_regularizer=l2(WEIGHT_DECAY))(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = LeakyReLU(0.1)(combined)\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', dtype='float32')(combined)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Enhanced data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    channel_shift_range=20.0\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Train',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'Train',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Create and compile model\n",
    "base_optimizer = AdamW(learning_rate=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "model = create_ensemble_model()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Enhanced callbacks (removed ModelCheckpoint due to issue)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "    BackupAndRestore(backup_dir='./backup')\n",
    "]\n",
    "\n",
    "# Gradient accumulation training function with manual checkpointing\n",
    "def train_with_gradient_accumulation(model, train_generator, validation_generator, steps_per_epoch, validation_steps, epochs, callbacks):\n",
    "    optimizer = model.optimizer\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    accum_steps = 4\n",
    "    effective_batch_size = BATCH_SIZE * accum_steps\n",
    "    best_val_acc = -float('inf')  # For manual checkpointing\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        train_iterator = iter(train_generator)\n",
    "        val_iterator = iter(validation_generator)\n",
    "        \n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            gradients = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "            step_train_loss = 0\n",
    "            \n",
    "            for _ in range(accum_steps):\n",
    "                try:\n",
    "                    x_batch, y_batch = next(train_iterator)\n",
    "                except StopIteration:\n",
    "                    train_iterator = iter(train_generator)\n",
    "                    x_batch, y_batch = next(train_iterator)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = model(x_batch, training=True)\n",
    "                    loss = loss_fn(y_batch, logits)\n",
    "                    scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "                \n",
    "                scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "                grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "                gradients = [g + acc_g for g, acc_g in zip(grads, gradients)]\n",
    "                step_train_loss += loss\n",
    "                train_acc_metric.update_state(y_batch, logits)\n",
    "            \n",
    "            step_train_loss /= accum_steps\n",
    "            gradients = [g / accum_steps for g in gradients]\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            total_train_loss += step_train_loss\n",
    "            \n",
    "            if (step + 1) % 10 == 0 or step == steps_per_epoch - 1:\n",
    "                print(f\"Step {step + 1}/{steps_per_epoch}, \"\n",
    "                      f\"Loss: {step_train_loss:.4f}, \"\n",
    "                      f\"Accuracy: {train_acc_metric.result():.4f}\")\n",
    "        \n",
    "        for val_step in range(validation_steps):\n",
    "            try:\n",
    "                x_val, y_val = next(val_iterator)\n",
    "            except StopIteration:\n",
    "                val_iterator = iter(validation_generator)\n",
    "                x_val, y_val = next(val_iterator)\n",
    "            \n",
    "            val_logits = model(x_val, training=False)\n",
    "            val_loss = loss_fn(y_val, val_logits)\n",
    "            total_val_loss += val_loss\n",
    "            val_acc_metric.update_state(y_val, val_logits)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / steps_per_epoch\n",
    "        avg_val_loss = total_val_loss / validation_steps\n",
    "        train_acc = train_acc_metric.result()\n",
    "        val_acc = val_acc_metric.result()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Summary: \"\n",
    "              f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Training Accuracy: {train_acc:.4f}, \"\n",
    "              f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Manual checkpointing\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save_weights('best_model.h5')\n",
    "            print(f\"Saved best model weights with validation accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        logs = {\n",
    "            'loss': float(avg_train_loss),\n",
    "            'accuracy': float(train_acc),\n",
    "            'val_loss': float(avg_val_loss),\n",
    "            'val_accuracy': float(val_acc)\n",
    "        }\n",
    "        for callback in callbacks:\n",
    "            callback.on_epoch_end(epoch, logs=logs)\n",
    "\n",
    "# Two-stage training with gradient accumulation\n",
    "print(\"Stage 1: Feature extraction\")\n",
    "train_with_gradient_accumulation(\n",
    "    model,\n",
    "    train_generator,\n",
    "    validation_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Fine-tuning\n",
    "print(\"Stage 2: Fine-tuning\")\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "base_optimizer = AdamW(learning_rate=BASE_LR/10, weight_decay=WEIGHT_DECAY)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "train_with_gradient_accumulation(\n",
    "    model,\n",
    "    train_generator,\n",
    "    validation_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS-30,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# [Rest of your code for TTA and evaluation remains unchanged]\n",
    "# ...\n",
    "print(\"Training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
